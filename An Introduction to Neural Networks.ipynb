{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Neural Networks\n",
    "\n",
    "\n",
    "* An Introduction to Probabilistic Graphical Models (Skip)\n",
    "    * Bayesian Networks\n",
    "    * Hidden Markov Chains\n",
    "* A review of linear regression\n",
    "    * linear algebra review\n",
    "        * row reduction of a matrix \n",
    "        * matrix multiplication\n",
    "        * invertability\n",
    "    * Implementing linear regression with linear algebra\n",
    "* Neural Networks\n",
    "    * Chain Rule over vector spaces\n",
    "    * psuedo code\n",
    "    * implementing a basic neural network in raw python\n",
    "        * Forward propagation\n",
    "        * Back propagation\n",
    "    * Issues in convergence\n",
    "        * vanishing gradient\n",
    "        * exploding gradient\n",
    "    * introduction to keras\n",
    "        * A simple example - regression\n",
    "        * A simple example - classification\n",
    "        * neural network architecture design\n",
    "            * final layer for regression\n",
    "            * final layer for classification\n",
    "            * regularization\n",
    "    * Surogate Models\n",
    "        * linear regression\n",
    "        * decision trees\n",
    "            * feature importance\n",
    "            * tre interpreter\n",
    "    * LIME\n",
    "        * regression example\n",
    "        * classification example\n",
    "    * SHAP\n",
    "        * regression example\n",
    "        * classification example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Review of Linear Regression\n",
    "\n",
    "In the first part of this tutorial we saw how to implement linear regression using calculus.  In order to build comfort with matrices, which are a central core notion in the design and implementation of neural networks, we will review how to implement linear regression, except this time, using the framing of linear algebra.\n",
    "\n",
    "The other reason it is helpful to recall our notions around linear regression is neural networks draw from linear regression.  The simplest possible neural network is actually just a set of chained linear regressions.  The power of neural networks is in how one does the chaining of course.  That said, neural nets can be far more complex, interesting, powerful and difficult to understand.  \n",
    "\n",
    "Finally, it's easy to get lost in all the machinery and choices of neural networks so it's important to ground them in context - neural networks just solve an optimization problem, minimizing error like any other algorithm.  And typically, the way they do that is via Stochastic Gradient Descent.\n",
    "\n",
    "## A Review of Stochastic Gradient Descent\n",
    "\n",
    "Recall the following picture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"NewtonIteration_Ani.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<img src=\"NewtonIteration_Ani.gif\">')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In it, we can see broadly how gradient descent works - we start with some initial guess, then we use the gradient (which is the same as a derivative) to update that guess and get closer to an optimum.  Optimums are the minimums or maximums of your function, depending on what you are trying to do.  In the case of gradient descent, you are trying to minimize loss, that is the difference between your predicted outcomes and the outcomes you saw in your training data.\n",
    "\n",
    "## A Review of Linear Algebra\n",
    "\n",
    "In the previous implementation of Gradient Descent we made use of the derivative directly via calculus to find our optimum, minimizing error.  However, in practice most algorithms make use of the linear algebra formulation of Gradient Descent because linear algebra is extremely power and we have many computational tricks to make it fast.\n",
    "\n",
    "Before we dive into the implementation, let's review some concepts from linear algebra to get a sense of how to work with matrices and vectors, the mainstays of the subfield.\n",
    "\n",
    "### Motivating Linear Algebra\n",
    "\n",
    "One of the general goals of linear algebra is to be able to solve sets of equations simultaneously.  First, let's see how to solve a single equation using simple python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'zero' for equation one is 0\n"
     ]
    }
   ],
   "source": [
    "def equation_one(x):\n",
    "    return x\n",
    "\n",
    "def find_zero_eq_one():\n",
    "    for i in range(-100, 100):\n",
    "        if equation_one(i) == 0:\n",
    "            return i\n",
    "        \n",
    "print(\"The 'zero' for equation one is\", find_zero_eq_one())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy enough!  This is the simplest possible way we could \"solve\" an equation, just guess and check.  Now let's see if this method get's more complex when we make our equation more sophisticated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'zero' for equation two is -7\n"
     ]
    }
   ],
   "source": [
    "def equation_two(x):\n",
    "    return x + 7\n",
    "\n",
    "def find_zero_eq_two():\n",
    "    for i in range(-100, 100):\n",
    "        if equation_two(i) == 0:\n",
    "            return i\n",
    "        \n",
    "print(\"The 'zero' for equation two is\", find_zero_eq_two())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've added a constant, thus changing our equation.  Still pretty simple and we can reuse our solver!  What happens when we go to two variables and add coeficients on our variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'zero' for equation four is (-9.400000000000002, 9.999999999999963)\n"
     ]
    }
   ],
   "source": [
    "def arange(start, stop, step):\n",
    "    iterator = start\n",
    "    while iterator < stop:\n",
    "        yield iterator\n",
    "        iterator += step\n",
    "\n",
    "def equation_four(x, y):\n",
    "    return 5*x + 4*y + 7\n",
    "\n",
    "def find_zero_eq_four(episolon):\n",
    "    for i in arange(-10, 10, 0.1):\n",
    "        for j in arange(-10, 10, 0.1):\n",
    "            if abs(equation_four(i, j)) < episolon:\n",
    "                return (i, j)\n",
    "        \n",
    "print(\"The 'zero' for equation four is\", find_zero_eq_four(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, now we need two for loops and also, we can no longer make use of the built-in range function.  Instead, we need access to a far more granular set of numbers - the floating points, instead of the integers.  This also means that we can no longer have equality in our stopping condition:\n",
    "\n",
    "`if abs(equation_four(i, j)) < episolon`\n",
    "\n",
    "Now we need set a tolerance for acceptance, a them of many data science algorithms, because typically we work with floating point numbers which (almost) never converge to an integer value.\n",
    "\n",
    "Let's add yet another variable just to drive the point home:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'zero' for equation four plus is (-9.8, -9.500000000000002, 9.999999999999963)\n"
     ]
    }
   ],
   "source": [
    "def arange(start, stop, step):\n",
    "    iterator = start\n",
    "    while iterator < stop:\n",
    "        yield iterator\n",
    "        iterator += step\n",
    "        \n",
    "def equation_four_plus(x, y, z):\n",
    "    return 5*x + 4*y + 8*z + 7\n",
    "\n",
    "def find_zero_eq_four_plus(episolon):\n",
    "    for i in arange(-10, 10, 0.1):\n",
    "        for j in arange(-10, 10, 0.1):\n",
    "            for k in arange(-10, 10, 0.1):\n",
    "                if abs(equation_four_plus(i, j, k)) < episolon:\n",
    "                    return (i, j, k)\n",
    "        \n",
    "print(\"The 'zero' for equation four plus is\", find_zero_eq_four_plus(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see things get even worse!  And now we need 3 for loops!  With every variable we add to a single equation, using classical techniques, like for loops, we will run into trouble fast.  Computationally speaking, linear algebra can make our lives significantly easier, producing algorithms that can solve systems of equations, that is many equations needing to be solved together much faster than the simple for loop scheme we have defined above.\n",
    "\n",
    "This is the reason no one uses calculus to solve gradient descent, the algorithm simply does not scale the same way.  And with a neural network, you may need to solve thousands or millions of equations simultaneously (typically hundreds).  \n",
    "\n",
    "Just for completeness, let's look at how to write down how to solve a single equation with an arbitrary number of variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-7.000000000000011,)\n",
      "(-10, 1.4999999999999816)\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import itertools\n",
    "\n",
    "\n",
    "def arange(start, stop, step):\n",
    "    iterator = start\n",
    "    while iterator < stop:\n",
    "        yield iterator\n",
    "        iterator += step\n",
    "\n",
    "        \n",
    "def equation_five(coefficients, constant, variables):\n",
    "    return sum([coefficients[index]*variables[index] \n",
    "                for index in range(len(variables))]) + constant\n",
    "\n",
    "\n",
    "def find_zero_eq_five(coefficients, constant, episolon):\n",
    "    eq_five = partial(equation_five, coefficients, constant)\n",
    "    value_range = list(arange(-10, 10, 0.1))\n",
    "    values = list(itertools.permutations(value_range, len(coefficients)))\n",
    "    for value in values:        \n",
    "        if abs(eq_five(value)) < episolon:\n",
    "                return value\n",
    "\n",
    "print(find_zero_eq_five([1], 7, 0.1))\n",
    "print(find_zero_eq_five([1, 2], 7, 0.1))\n",
    "#print(find_zero_eq_five([1, 2, 3], 7, 0.1))\n",
    "#print(find_zero_eq_five([1, 2, 3, 4], 7, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason this solver looks a little more compact is not because it's solving fewer equations, it's because it's getting all the for loops upfront via the permutations function.  This little computational trick makes things a little more tractable computationally speaking, but it doesn't give us enough of a boost to make things tenable.  I could in fact break this notebook just by running the third or fourth example.\n",
    "\n",
    "## Matrices\n",
    "\n",
    "We've worked with a few algorithms a lot.  Now we'll introduce a new data structure - the matrix.  A matrix mathematically speaking is a notational convention.  More or less a matrix is just a vector of vectors.  And a vector is just the coefficients of a linear equation.  We've been implicitly using mathematical vectors since we introduced equations above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first example of matrices, Supply and Demand\n",
    "\n",
    "Let's say that we own and operate a business that sells candy bars.  Assume there is implicit demand for candy bars and many other candy bar sellers.  However, assume our costs are perhaps slightly different from other candy bar sellers, so we can choose what price to sell our candy bar at, subject to our ability to supply candy.  We can also assume we have some supply control because most people aren't patient about candy, they want it when they want it.  So if someone happens by your shop, the price may actually effect their decision to buy the candy or not.\n",
    "\n",
    "So let's say you have a supply function like:\n",
    "\n",
    "`price = 2*quantity + 3`\n",
    "\n",
    "What this equation says is as a business owner, as the price increases you'll want to supply more candy bars, because the perception is, you'll make more money.  \n",
    "\n",
    "Let's also say there is a demand function like:\n",
    "\n",
    "`price = 3 - 2*quantity`\n",
    "\n",
    "This equation says the price you can charge is subject to how many candy bars you make.  So if your shop is filled to the brim and your the only local vendor around, folks will likely be able to always satisfy their craving for candy.  And if you only make a single candy bar, folks will be willing to pay a lot to be the only one to get it.  Because candy is delicious.\n",
    "\n",
    "Given the above equations we need both the supply and demand equation to find the unique equilibrium of price and quantity such that the market for candy bars is in balance.  This balancing point, all other things held constant, will be the price that maximizes utility for the buyers of candy bars as well as the sellers of candy bars.  And since we are treating our example as idealized, we are in a capitalism regime where the intention is to maximize utility for all.  However, even with the introduction of immorality and therefore businesses or agents acting in subversive ways, all economies are still subject to the above laws generally speaking and therefore understanding this idealized situation still yields value.\n",
    "\n",
    "So!  Now that we have our supply and demand equations, how do we find the unique price, quantity that yields our equilibrium?  With matrices of course!\n",
    "\n",
    "First we'll need to turn these equations into the appropriate form:\n",
    "\n",
    "Supply:\n",
    "\n",
    "`price - 2*quantity = 3`\n",
    "\n",
    "Demand:\n",
    "\n",
    "`price + 2*quantity = 3`\n",
    "\n",
    "We need to do this so all the variables are on one side of the equation.  Now we can do this:\n",
    "\n",
    "Supply/Demand Matrix:\n",
    "\n",
    "$$\n",
    "  \\begin{bmatrix} 1 & -2 \\\\ 1 & 2 \\end{bmatrix} \n",
    "  \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now we have the two resulting mathematical objects, a matrix describing the coeficients of the two equations and a vector describing the solution space.  We'll do something called row reduction on the matrix which will produce unique solutions for the equations.  Before we carry out these operations notice that if our matrix looks like this:\n",
    "\n",
    "$$\n",
    "  \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \n",
    "  \\begin{bmatrix} a \\\\ b \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Then whatever our variables are, they are equal to a and b respectively.  Because the matrix:\n",
    "\n",
    "$$\n",
    "  \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \n",
    "$$\n",
    "\n",
    "Translates to\n",
    "\n",
    "$$\n",
    "  \\begin{bmatrix} x \\\\ y \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "With the `[a b]` vector this becomes:\n",
    "\n",
    "$$\n",
    "x = a \\\\\n",
    "y = b\n",
    "$$\n",
    "\n",
    "Which solves x and y uniquely!  So now we'll do row reduction to create a matrix which looks like the above one.  And then we'll apply the changes we used on the matrix, onto the solution vector as well.  Which will in turn solve our system of equations!\n",
    "\n",
    "\n",
    "$\\begin{bmatrix} 1 & -2 \\\\ 1 & 2 \\end{bmatrix} $\n",
    "\n",
    "```\n",
    "(R[1] = R[1] + R[2])\n",
    "------------------->\n",
    "``` \n",
    "\n",
    "$ \\begin{bmatrix} 2 & 0 \\\\ 1 & 2 \\end{bmatrix} $\n",
    "\n",
    "```\n",
    "(R[1] = R[1]/2)\n",
    "------------------->\n",
    "``` \n",
    "\n",
    "$ \\begin{bmatrix} 1 & 0 \\\\ 1 & 2 \\end{bmatrix} $\n",
    "\n",
    "```\n",
    "(R[2] = R[2] - R[1])\n",
    "------------------->\n",
    "``` \n",
    "\n",
    "$ \\begin{bmatrix} 1 & 0 \\\\ 0 & 2 \\end{bmatrix} $\n",
    "\n",
    "```\n",
    "(R[2] = R[2]/2)\n",
    "------------------->\n",
    "``` \n",
    "\n",
    "$ \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} $\n",
    "\n",
    " \n",
    "Now we'll simply apply the changes to the matrix to the vector:\n",
    "\n",
    "$ \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix} $\n",
    "\n",
    "```\n",
    "(R[1] = R[1] + R[2])\n",
    "------------------->\n",
    "``` \n",
    "\n",
    "$ \\begin{bmatrix} 6 \\\\ 3 \\end{bmatrix} $\n",
    "\n",
    "```\n",
    "(R[1] = R[1]/2)\n",
    "------------------->\n",
    "```\n",
    "\n",
    "$ \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix} $\n",
    "\n",
    "```\n",
    "(R[2] = R[2] - R[1])\n",
    "------------------->\n",
    "```\n",
    "\n",
    "$ \\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix} $\n",
    "\n",
    "```\n",
    "(R[2] = R[2]/2)\n",
    "------------------->\n",
    "```\n",
    "\n",
    "$ \\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix} $\n",
    "\n",
    "\n",
    "So the unique solution is:\n",
    "\n",
    "```\n",
    "price = 3\n",
    "quantity = 0\n",
    "```\n",
    "\n",
    "What this ends up telling us is it's not possible to optimally operate our candy bar shop.  Which is pretty sad.  So this means we shouldn't open our candy bar shop.  Or we need to change our supply equation by becoming more efficient at producing candy bars.  Of course, it could also be the case that this is truly optimal because candy is bad for you :P.  But that's not really an economic analysis.\n",
    "\n",
    "In any event, we are now ready to write a program which does row reduction for us, by solving a bunch of very simple equations.  Notice, what we really want is to make the numbers in the matrix that are not on the diagonal zero.  And then we want to make the numbers that are on the diagonal 1.  And then we need to store those operations and apply them to solution matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0, -2.0], [0.5, 1.0]]\n",
      "1 0\n",
      "[[1.0, -2.0], [0.0, 2.0]]\n",
      "0 1\n",
      "[3.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from random import random\n",
    "from sys import setrecursionlimit\n",
    "\n",
    "def eq_solver(val, func):\n",
    "    if func(val) > 0:\n",
    "        val -= 1\n",
    "        return eq_solver(val, func)\n",
    "    elif func(val) < 0:\n",
    "        val += 1\n",
    "        return eq_solver(val, func)\n",
    "    else:\n",
    "        return round(val, 5)\n",
    "    \n",
    "    \n",
    "def eq_float_solver(val, eq, epsilon=0.0001, step_size=0.001, debug=False):\n",
    "    if abs(eq(val)) < epsilon:\n",
    "        return round(val, 5)\n",
    "    elif eq(val) > 0:\n",
    "        if debug:\n",
    "            val -= step_size\n",
    "            return eq_float_solver(val, eq)\n",
    "        else:\n",
    "            val -= step_size\n",
    "            return eq_float_solver(val, eq)\n",
    "    elif eq(val) < 0:\n",
    "        if debug:\n",
    "            val += step_size\n",
    "            return eq_float_solver(val, eq)\n",
    "        else:\n",
    "            val += step_size\n",
    "            return eq_float_solver(val, eq)\n",
    "    else:\n",
    "        return round(val, 5)\n",
    "\n",
    "\n",
    "def eq_diag_solver(val, eq, epsilon=0.0001, step_size=0.001):\n",
    "    if abs(eq(val)) - 1 < epsilon:\n",
    "        return round(val, 5)\n",
    "    elif eq(val) > 1:\n",
    "        val -= step_size\n",
    "        if val == 0:\n",
    "            val -= step_size\n",
    "        return eq_float_solver(val, eq)\n",
    "    elif eq(val) < 1:\n",
    "        val += step_size\n",
    "        if val == 0:\n",
    "            val += step_size\n",
    "        return eq_float_solver(val, eq)\n",
    "    else:\n",
    "        return round(val, 5)\n",
    "    \n",
    "\n",
    "def arange(start, stop, step):\n",
    "    cur = start\n",
    "    while start < stop:\n",
    "        yield cur\n",
    "        cur += step\n",
    "\n",
    "        \n",
    "def iterative_solver(eq, start, stop, epsilon=0.0001, step_size=0.001):\n",
    "    for val in arange(start, stop, step_size):\n",
    "        if abs(eq(val)) < epsilon:\n",
    "            return round(val, 5)\n",
    "\n",
    "\n",
    "def flatten(matrix):\n",
    "    listing = []\n",
    "    for row in matrix:\n",
    "        for elem in row:\n",
    "            listing.append(elem)\n",
    "    return listing\n",
    "\n",
    "\n",
    "def solve_matrix(matrix):\n",
    "    flattened_matrix = flatten(matrix)\n",
    "    largest_value = max(flattened_matrix)\n",
    "    num_zeros = len(str(largest_value))\n",
    "    val_range = int(\"1\" + \"0\"*num_zeros)\n",
    "    steps = []\n",
    "    cur_matrix = matrix\n",
    "    for index in range(len(matrix)):\n",
    "        col_index = index\n",
    "        row_index = index\n",
    "        cur_matrix, step = solve_diag(cur_matrix[col_index][row_index],\n",
    "                                      row_index, col_index, cur_matrix, val_range)\n",
    "        steps.append(step)\n",
    "    for row_index in range(len(matrix[0])):\n",
    "        for col_index in range(len(matrix)):\n",
    "            if col_index == row_index:\n",
    "                continue\n",
    "            print(cur_matrix)\n",
    "            step = solve_vector(cur_matrix[col_index][row_index], \n",
    "                                row_index, col_index, cur_matrix, val_range)\n",
    "            cur_matrix = linear_combination(cur_matrix, step)\n",
    "            print(col_index, row_index)\n",
    "            steps.append(step)\n",
    "    return steps, cur_matrix\n",
    "\n",
    "\n",
    "def linear_combination(matrix, step):\n",
    "    update_row = step[0]\n",
    "    other_row = step[1]\n",
    "    transformer = step[2]\n",
    "    for index in range(len(matrix[0])):\n",
    "        matrix[update_row][index] = transformer(\n",
    "            matrix[update_row][index], matrix[other_row][index])\n",
    "    return matrix\n",
    "    \n",
    "\n",
    "def solve_diag(elem, cur_elem_idx, diag_index, matrix, magnitude):\n",
    "    reciprical = 1/elem\n",
    "    matrix[diag_index] = [elem*reciprical \n",
    "                          for elem in matrix[diag_index]]\n",
    "    operation = lambda coef, elem: elem*coef\n",
    "    op = partial(operation, reciprical)\n",
    "    step = [diag_index, cur_elem_idx, op, None, reciprical, \"rescale\"]\n",
    "    return matrix, step\n",
    "\n",
    "\n",
    "def solve_vector(elem, cur_elem_idx, diag_index, matrix, magnitude):\n",
    "    for row_index, matrix_row in enumerate(matrix):        \n",
    "        if matrix_row[cur_elem_idx] != 0 and row_index != diag_index:\n",
    "            other_elem = matrix_row[cur_elem_idx]\n",
    "            other_row = row_index\n",
    "            break\n",
    "    eq_to_solve = lambda elem, other_elem, coef: elem + coef*other_elem\n",
    "    to_solve = partial(eq_to_solve, elem, other_elem)\n",
    "    start, stop = magnitude*-1, magnitude\n",
    "    coef = iterative_solver(to_solve, start=start, stop=stop)\n",
    "    operation = lambda coef, elem, other: elem + coef*other\n",
    "    op = partial(operation, coef)\n",
    "    return [diag_index, cur_elem_idx, op, other_row, coef, \"linear_combo\"]\n",
    "\n",
    "\n",
    "def apply_steps(vector, steps):\n",
    "    for step in steps:\n",
    "        if step[-1] == \"linear_combo\":\n",
    "            idx = step[0]\n",
    "            other_idx = step[3]\n",
    "            coef = step[4]\n",
    "            vector[idx] = vector[idx] + coef*vector[other_idx]\n",
    "        else:\n",
    "            idx = step[0]\n",
    "            coef = step[4]\n",
    "            vector[idx] = vector[idx]*coef\n",
    "    return vector\n",
    "\n",
    "\n",
    "#setrecursionlimit(10000)\n",
    "if __name__ == '__main__':\n",
    "    matrix = [[1, -2], [1, 2]]\n",
    "    steps, cur_matrix = solve_matrix(matrix)\n",
    "    print(apply_steps([3, 3], steps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I present the above solution for completeness, in case folks are interested in how to do this from scratch.  \n",
    "\n",
    "## Matrix Operations (Digression)\n",
    "\n",
    "Now that we've seen the power of matrices to allow us to solve systems of equations, let's briefly talk about some of their operations:\n",
    "\n",
    "* Addition\n",
    "* Multiplication\n",
    "\n",
    "Matrix Addition is as simple as you'd suspect, simply go across and add each element in the same row column position:\n",
    "\n",
    "$$\n",
    "  \\begin{bmatrix} 1 & -2 \\\\ 1 & 2 \\end{bmatrix} \n",
    "  + \n",
    "  \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}\n",
    "  =\n",
    "  \\begin{bmatrix} 2 & -1 \\\\ 2 & 3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Notice, the matrices are of the same size, and must be!  So that means you can't add two matrices of different shapes!\n",
    "\n",
    "Next let's look at multiplication:\n",
    "\n",
    "Matrix Multiplication is far more complicated, you have to go across the ith row and jth column to get the i,j entry.  This is best seen with an example:\n",
    "\n",
    "$$\n",
    "  \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \n",
    "  * \n",
    "  \\begin{bmatrix} 2 & 1 \\\\ 4 & 3 \\end{bmatrix}\n",
    "  =\n",
    "  \\begin{bmatrix} 10 & 7 \\\\ 22 & 15 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In order to recover the position at index 1, 1 (matrices are indexed starting at 1 in mathematical notation), you'd need to do:\n",
    "\n",
    "$$ 1*2 + 2*4 = 10 $$\n",
    "\n",
    "To make it clear what happens you take the first row of the first matrix: 1, 2 and multiply it by the first column of the second matrix: 2, 4 element by element and then sum the two multiplications.\n",
    "\n",
    "This means that matrix multiplication is not communitive!  Which means:\n",
    "\n",
    "$$ A*B \\neq B* A $$\n",
    "\n",
    "For matrices A and B.\n",
    "\n",
    "Additionally, matrix mulitplication need not operate over matrices of the exact same size.  Therefore we can do things like this:\n",
    "\n",
    "$$\n",
    "  \\begin{bmatrix} 1 & 2 & 3\\\\ 3 & 4 & 5\\end{bmatrix} \n",
    "  * \n",
    "  \\begin{bmatrix} 2 & 1 \\\\ 4 & 3 \\\\ 5 & 6 \\end{bmatrix}\n",
    "  =\n",
    "  \\begin{bmatrix} 25 & 25 \\\\ 47 & 45 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The only requirement is that our matrices be composable, that is, the number of columns of the first matrix is the same as the number of rows as the second matrix.  It's very important to understand how to compose matrices of different shapes because our individual layers of our neural network are in fact matrices.  If you try to compose two layers that can't be composed you will get a very annoying and confusing error.\n",
    "\n",
    "For completeness, let's see how to do this in python!  Here we'll make use of numpy because the code for implementing fast matrix mulitplication is very confusing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25 25]\n",
      " [47 45]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([[1, 2, 3], [3, 4, 5]])\n",
    "B = np.array([[2, 1], [4, 3], [5, 6]])\n",
    "print(np.dot(A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completeness, here is the underlying multiplication written out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 7]\n",
      "[22, 15]\n"
     ]
    }
   ],
   "source": [
    "class Matrix:\n",
    "    def __init__(self,matrix):\n",
    "        self.matrix = matrix\n",
    "            \n",
    "    def get_size(self):\n",
    "        return len(self.matrix),len(self.matrix[0])\n",
    "\n",
    "    def pprint(self):\n",
    "        for row in self.matrix:\n",
    "            print(row)\n",
    "                \n",
    "        \n",
    "    def to_array(self):\n",
    "        return self.matrix\n",
    "        \n",
    "    def get_elem(self,row,col):\n",
    "        return self.matrix[row][col]    \n",
    "        \n",
    "    def __add__(self,other):\n",
    "        row_size,col_size = self.get_size()\n",
    "        new_matrix = []\n",
    "        for row in range(row_size):\n",
    "            new_matrix.append([elem+other.matrix[row][ind] for ind,elem in enumerate(self.matrix[row])])\n",
    "        return Matrix(new_matrix)\n",
    "    \n",
    "    def __sub__(self,other):\n",
    "        row_size,col_size = self.get_size()\n",
    "        new_matrix = []\n",
    "        for row in range(row_size):\n",
    "            new_matrix.append([elem-other.matrix[row][ind] for ind,elem in enumerate(self.matrix[row])])\n",
    "        return Matrix(new_matrix)\n",
    "\n",
    "    def simple_multiplication(self,A,B):\n",
    "        row_size,col_size = self.get_size()\n",
    "        new_matrix = [[0 for i in range(row_size)] for j in range(row_size)]\n",
    "        for i in range(row_size):\n",
    "            for k in range(row_size):\n",
    "                for j in range(row_size):\n",
    "                    new_matrix[i][j] += A[i][k] * B[k][j]\n",
    "        return Matrix(new_matrix)\n",
    "    \n",
    "    def __mul__(self,other):\n",
    "        row_size,col_size = self.get_size()\n",
    "        if row_size <= 2:\n",
    "            return self.simple_multiplication(self.matrix,other.matrix)\n",
    "        else:\n",
    "            new_size = row_size//2\n",
    "            A = [[0 for j in range(new_size)] for i in range(new_size)]\n",
    "            B = [[0 for j in range(new_size)] for i in range(new_size)]\n",
    "            C = [[0 for j in range(new_size)] for i in range(new_size)]\n",
    "            D = [[0 for j in range(new_size)] for i in range(new_size)]\n",
    "\n",
    "            E = [[0 for j in range(new_size)] for i in range(new_size)]\n",
    "            F = [[0 for j in range(new_size)] for i in range(new_size)]\n",
    "            G = [[0 for j in range(new_size)] for i in range(new_size)]\n",
    "            H = [[0 for j in range(new_size)] for i in range(new_size)]\n",
    "\n",
    "            for i in range(new_size):\n",
    "                for j in range(new_size):\n",
    "                    A[i][j] = self.matrix[i][j]\n",
    "                    B[i][j] = self.matrix[i][j+new_size]\n",
    "                    C[i][j] = self.matrix[i + new_size][j]\n",
    "                    D[i][j] = self.matrix[i + new_size][j + new_size]\n",
    "\n",
    "                    E[i][j] = other.matrix[i][j]\n",
    "                    F[i][j] = other.matrix[i][j+new_size]\n",
    "                    G[i][j] = other.matrix[i + new_size][j]\n",
    "                    H[i][j] = other.matrix[i + new_size][j + new_size]\n",
    "\n",
    "            A = Matrix(A)\n",
    "            B = Matrix(B)\n",
    "            C = Matrix(C)\n",
    "            D = Matrix(D)\n",
    "            E = Matrix(E)\n",
    "            F = Matrix(F)\n",
    "            G = Matrix(G)\n",
    "            H = Matrix(H)\n",
    "            \n",
    "            p1 = A*(F-H)\n",
    "            p2 = (A+B)*H\n",
    "            p3 = (C+D)*E\n",
    "            p4 = D*(G-E)\n",
    "            p5 = (A+D)*(E+H)\n",
    "            p6 = (B-D)*(G+H)\n",
    "            p7 = (A -C)*(E+F)\n",
    "\n",
    "            c11 = p5 + p4 - p2 + p6\n",
    "            c12 = p1 + p2\n",
    "            c21 = p3 + p4\n",
    "            c22 = p1+ p5 - p3 - p7\n",
    "\n",
    "            final = [[0 for j in range(row_size)] for i in range(row_size)]\n",
    "            for i in range(new_size):\n",
    "                for j in range(new_size):\n",
    "                    final[i][j] = c11.matrix[i][j]\n",
    "                    final[i][j+new_size] = c12.matrix[i][j]\n",
    "                    final[i + new_size][j] = c21.matrix[i][j]\n",
    "                    final[i + new_size][j + new_size] = c22.matrix[i][j]\n",
    "            return Matrix(final)\n",
    "            \n",
    "A = Matrix([[1,2], [3,4]])\n",
    "B = Matrix([[2, 1], [4, 3]])\n",
    "(A*B).pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invertability\n",
    "\n",
    "Now that we've seen how to multiply two matrices, a natural question is, can we invert a given matrix to recover the identity, if we could, then we'd have a succinct way to solve systems of equations!  It turns out, not all matrices are invertable, but some are!  And those that are make finding the solutions for a system of equations, extremely easy!  We need not go through all the row reduction stuff and we can still solve our equations.\n",
    "\n",
    "The notation for inverting a matrix is typically as follows:\n",
    "\n",
    "$$ A * A^{-1} = I $$\n",
    "\n",
    "Where A is some invertable matrix, $A^{-1}$ is it's inverse and $I$ is the identity matrix. \n",
    "\n",
    "Let's see how to obtain the inverse for a square matrix of size 2x2:\n",
    "\n",
    "$$\n",
    "  \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}^{-1}\n",
    "  =\n",
    "  \\frac{1}{ad - bc}\n",
    "  *\n",
    "  \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note, not all matrices are invertable (even square ones).  Those are called singular matrices.  You'll see this sort of error come up _a lot_ in machine learning when you are trying to fit a model to some data.\n",
    "\n",
    "## Linear Regression w/ Linear Algebra\n",
    "\n",
    "Now that we've come up with a set of tools for doing linear algebra, let's apply them to show how we can implement gradient descent with our new tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate some data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "X = 7 * np.random.rand(100,1)\n",
    "y = 9 + 4 * X+np.random.randn(100,1)\n",
    "\n",
    "plt.plot(X,y,'b.')\n",
    "plt.xlabel(\"$x$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "_ =plt.axis([0,5,6,40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta0:          9.016,\n",
      "Theta1:          3.929\n",
      "Final cost/MSE:  40.881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 5, 6, 40]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cal_cost(theta,X,y):\n",
    "    '''\n",
    "    \n",
    "    Calculates the cost for given X and Y. The following shows and example of a single dimensional X\n",
    "    theta = Vector of thetas \n",
    "    X     = Row of X's np.zeros((2,j))\n",
    "    y     = Actual y's np.zeros((2,1))\n",
    "    \n",
    "    where:\n",
    "        j is the no of features\n",
    "    '''\n",
    "    \n",
    "    m = len(y)\n",
    "    \n",
    "    predictions = X.dot(theta)\n",
    "    cost = (1/2*m) * np.sum(np.square(predictions-y))\n",
    "    return cost\n",
    "\n",
    "def stocashtic_gradient_descent(X,y,theta,learning_rate=0.01,iterations=10):\n",
    "    '''\n",
    "    X    = Matrix of X with added bias units\n",
    "    y    = Vector of Y\n",
    "    theta=Vector of thetas np.random.randn(j,1)\n",
    "    learning_rate \n",
    "    iterations = no of iterations\n",
    "    \n",
    "    Returns the final theta vector and array of cost history over no of iterations\n",
    "    '''\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros(iterations)\n",
    "    \n",
    "    \n",
    "    for it in range(iterations):\n",
    "        cost =0.0\n",
    "        for i in range(m):\n",
    "            rand_ind = np.random.randint(0,m)\n",
    "            X_i = X[rand_ind,:].reshape(1,X.shape[1])\n",
    "            y_i = y[rand_ind].reshape(1,1)\n",
    "            prediction = np.dot(X_i,theta)\n",
    "\n",
    "            theta = theta -(1/m)*learning_rate*( X_i.T.dot((prediction - y_i)))\n",
    "            cost += cal_cost(theta,X_i,y_i)\n",
    "        cost_history[it]  = cost\n",
    "        \n",
    "    return theta, cost_history\n",
    "\n",
    "lr =0.5\n",
    "n_iter = 50\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "X_b = np.c_[np.ones((len(X),1)),X]\n",
    "theta,cost_history = stocashtic_gradient_descent(X_b,y,theta,lr,n_iter)\n",
    "\n",
    "print('Theta0:          {:0.3f},\\nTheta1:          {:0.3f}'.format(theta[0][0],theta[1][0]))\n",
    "print('Final cost/MSE:  {:0.3f}'.format(cost_history[-1]))\n",
    "\n",
    "X_new = np.array([[0],[5]])\n",
    "X_new_b = np.c_[np.ones((2,1)),X_new]\n",
    "y_predict = X_new_b.dot(theta)\n",
    "y_predict\n",
    "\n",
    "plt.plot(X_new,y_predict,'r-')\n",
    "plt.plot(X,y,'b.')\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([0,5,6,40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we've now added in matrix multiplications into our stochastic gradient descent.  I won't go over the code in detail, since it's basically conceptually the same thing as before.  It's worth noting the change in operations.  I leave it as an exercise to the reader to compare the two sets of code.\n",
    "\n",
    "## Neural Networks\n",
    "\n",
    "### A naive explanation\n",
    "\n",
    "Neural networks in a sense really aren't anything new \"mathematically\".  All we are really doing is applying the chain rule over a vector space ( to do back propagation ).\n",
    "\n",
    "Let's see a simple example of chain rules:\n",
    "\n",
    "$$ f(x) = (log(x))^{2} $$\n",
    "\n",
    "To do the derivative we need to do:\n",
    "\n",
    "$$ f'(g(x)) = f'(g(x))* g'(x) $$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$ f'(x) = 2 * log(x) * 1/x $$\n",
    "\n",
    "So if our functions are over matrices then we are doing neural networks :D\n",
    "\n",
    "### Psuedo Code\n",
    "\n",
    "There are two main algorithms in neural networks:\n",
    "\n",
    "* forward propagation ( predict )\n",
    "* back propagation ( train )\n",
    "\n",
    "\n",
    "![](nn_psuedocode.png)\n",
    "\n",
    "[source](https://en.wikipedia.org/wiki/Artificial_neural_network)\n",
    "\n",
    "\n",
    "As you can see the basic idea is:\n",
    "\n",
    "* propagate data through a set of functions which are joined together via matrix multiplication\n",
    "\n",
    "* At the last layer look at how much your prediction was off by\n",
    "\n",
    "* update your weight matrix by the gradient * the error (this is a simplification)\n",
    "\n",
    "* do this until your error is lower than some tolerance\n",
    "\n",
    "\n",
    "## A Naive Implementation of a Neural Network\n",
    "\n",
    "Now that we understand the basic notions, let's go ahead and look at a simple example in Python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum error for the this network was 0.12715212791542269\n",
      "The average error for the this network was 0.18068367228640816\n",
      "These were the inflection points for  0\n",
      "There were 0 in total\n",
      "The minimum error for the this network was 0.0029750529544725467\n",
      "The average error for the this network was 0.07204178609933273\n",
      "These were the inflection points for  1\n",
      "There were 0 in total\n",
      "The minimum error for the this network was 0.0017568698739491642\n",
      "The average error for the this network was 0.05579115465629704\n",
      "These were the inflection points for  2\n",
      "There were 0 in total\n",
      "The minimum error for the this network was 0.0018026102957684298\n",
      "The average error for the this network was 0.07386961361375748\n",
      "These were the inflection points for  3\n",
      "There were 0 in total\n",
      "The minimum error for the this network was 0.0017049973593299145\n",
      "The average error for the this network was 0.0816752845275053\n",
      "These were the inflection points for  4\n",
      "There were 0 in total\n",
      "The minimum error for the this network was 0.0014786023462123967\n",
      "The average error for the this network was 0.06691266329063701\n",
      "These were the inflection points for  5\n",
      "There were 0 in total\n",
      "The minimum error for the this network was 0.0016535667120873266\n",
      "The average error for the this network was 0.06031122048123905\n",
      "These were the inflection points for  6\n",
      "There were 0 in total\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is a vanilla neural network, written from scratch by me.  I adapted this from a series of blog posts on neural networks:\n",
    "\n",
    "http://iamtrask.github.io/2015/07/12/basic-python-network/\n",
    "\n",
    "You'll notice that this network is a generalization of the one found in the above blog post\n",
    "\n",
    "How to use:\n",
    "\n",
    "Example:\n",
    "\n",
    "num_hidden_nodes = 5\n",
    "X = np.array([[1,1],\n",
    "              [1,0],\n",
    "              [0,1],\n",
    "              [0,0]])\n",
    "                \n",
    "y = np.array([[0],[1],[1],[1]])\n",
    "run_once(num_hidden_nodes,X,y)\n",
    "\n",
    "The above example creates a neural network with 5 hidden nodes, X is the indepent variables, where is y the dependent variables.\n",
    "Note these are matrices - meaning I'm assuming a multi-dimensional array\n",
    "If you run this example from the command line:\n",
    "\n",
    "python back_prop.py \n",
    "\n",
    "it will run the the run once method.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "def check_index(cur_index,size):\n",
    "    if size - abs(cur_index) > 0:\n",
    "        return \"keep going\"\n",
    "    else:\n",
    "        return \"stop\"\n",
    "\n",
    "def rate_of_change(a,b):\n",
    "    try:\n",
    "        return float(b-a)/2\n",
    "    except:\n",
    "        return 0 \n",
    "        \n",
    "def rates_of_change(listing):\n",
    "    listing2 = listing[1:]\n",
    "    return list(map(rate_of_change, listing, listing2))[:-1]\n",
    "\n",
    "def check_sign(num):\n",
    "    if num >= 0:\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"negative\"\n",
    "    \n",
    "def find_inflection_points(listing):\n",
    "    rates = rates_of_change(listing)\n",
    "    inflection_points = []\n",
    "    num_inflection_points = 0\n",
    "    sign = check_sign(rates[0]) \n",
    "    for index,rate in enumerate(rates):\n",
    "        new_sign = check_sign(rate)\n",
    "        if new_sign != sign:\n",
    "            inflection_points.append(rate)\n",
    "            num_inflection_points += 1\n",
    "            sign = new_sign\n",
    "    return inflection_points,num_inflection_points\n",
    "\n",
    "def get_hidden_layer_index(nn):\n",
    "    indexes = []\n",
    "    for index,syn in enumerate(nn):\n",
    "        if type(syn[\"name\"]) == type(int()):\n",
    "            indexes.append(index)\n",
    "    return indexes\n",
    "\n",
    "def sort_by_key(listing,sort_by):\n",
    "    \"\"\"\n",
    "    Expects a list of dictionaries \n",
    "    Returns a list of dictionaries sorted by the associated key\n",
    "    \"\"\"\n",
    "    keys = []\n",
    "    translate = {}\n",
    "    for ind,elem in enumerate(listing):\n",
    "        key = elem[sort_by]\n",
    "        translate[key] = ind\n",
    "        keys.append(key)\n",
    "    keys.sort()\n",
    "    new_ordering = [translate[key] for key in keys]\n",
    "    return [listing[i] for i in new_ordering]\n",
    "\n",
    "def nonlin(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def create_connection(num_rows,num_cols):\n",
    "    return 2*np.random.random((num_rows,num_cols)) -1\n",
    "\n",
    "def create_nn(input_data,output_data,num_hidden_layers):\n",
    "    nn = [{\"name\":\"input data\",\"connection\":input_data}]\n",
    "    #input layer\n",
    "    input_syn = {\"name\":\"input layer\"}\n",
    "    input_syn[\"connection\"] = create_connection(len(input_data[0]),len(input_data))\n",
    "    nn.append(input_syn)\n",
    "    #hidden layers\n",
    "    for i in range(num_hidden_layers):\n",
    "        syn = {\"name\":i}\n",
    "        syn[\"connection\"] = create_connection(len(input_data),len(input_data))\n",
    "        nn.append(syn)\n",
    "    #output_layer\n",
    "    syn = {\"name\":\"output layer\"}\n",
    "    syn[\"connection\"] = create_connection(len(output_data),len(output_data[0]))\n",
    "    nn.append(syn)\n",
    "    nn.append({\"name\":\"output data\",\"connection\":output_data})\n",
    "    return nn\n",
    "\n",
    "\n",
    "def forward_propagate(synapses):\n",
    "    layers = [synapses[0][\"connection\"]]\n",
    "    for ind,synapse in enumerate(synapses[:-1]):\n",
    "        if ind == 0: continue\n",
    "        layers.append(\n",
    "            nonlin(np.dot(layers[ind-1],synapse[\"connection\"]))\n",
    "        )\n",
    "    return layers\n",
    "\n",
    "def back_propagate(layers,synapses):\n",
    "    errors = [synapses[-1][\"connection\"] - layers[-1]]\n",
    "    synapses_index = -1\n",
    "    layers_index = -1\n",
    "    errors_index = 0\n",
    "    deltas_index = 0\n",
    "    deltas = []\n",
    "    while len(layers) - abs(layers_index) > 0:\n",
    "        deltas.append(errors[errors_index]*nonlin(layers[layers_index],deriv=True))\n",
    "        synapses_index -= 1\n",
    "        layers_index -= 1\n",
    "        errors.append(deltas[deltas_index].dot(synapses[synapses_index][\"connection\"].T))\n",
    "        errors_index += 1\n",
    "        deltas_index += 1\n",
    "    synapses_index = -2\n",
    "    layers_index = -2\n",
    "    deltas_index = 0\n",
    "    while len(layers) - abs(layers_index) >= 0:\n",
    "        synapses[synapses_index][\"connection\"] += layers[layers_index].T.dot(deltas[deltas_index])\n",
    "        synapses_index -= 1\n",
    "        layers_index -= 1\n",
    "        deltas_index += 1\n",
    "    return synapses,errors[0]\n",
    "            \n",
    "\n",
    "def run_once(num_hidden_nodes,X,y):\n",
    "    np.random.seed(1)\n",
    "    errors = []\n",
    "    nn = create_nn(X,y,num_hidden_nodes)\n",
    "    for j in range(70000):\n",
    "        layers = forward_propagate(nn)\n",
    "        nn,error = back_propagate(layers,nn)\n",
    "        if j%10000 == 0:\n",
    "            errors.append(np.mean(np.abs(error)))\n",
    "    return errors\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    #tune(11)\n",
    "    X = np.array([[1,1],\n",
    "                  [1,0],\n",
    "                  [0,1],\n",
    "                  [0,0]])\n",
    "                \n",
    "    y = np.array([[0],\n",
    "                  [1],\n",
    "                  [1],\n",
    "                  [1]])\n",
    "    for i in range(0,7):\n",
    "        errors = run_once(i,X,y)\n",
    "        print(\"The minimum error for the this network was\",min(errors))\n",
    "        print(\"The average error for the this network was\",sum(errors)/float(len(errors)))\n",
    "        inflection_points,num_inflection_points = find_inflection_points(errors)\n",
    "        print(\"These were the inflection points for \",i)\n",
    "        print(\"There were\",num_inflection_points,\"in total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up our network\n",
    "\n",
    "As you can see from the above our naive neural network has three steps:\n",
    "\n",
    "* Initialization\n",
    "* forward propagation (guess)\n",
    "* back propagation (check)\n",
    "\n",
    "The initialization step sets up our neural network and the general architecture:\n",
    "\n",
    "```python\n",
    "def create_connection(num_rows,num_cols):\n",
    "    return 2*np.random.random((num_rows,num_cols)) -1\n",
    "\n",
    "def create_nn(input_data,output_data,num_hidden_layers):\n",
    "    nn = [{\"name\":\"input data\",\"connection\":input_data}]\n",
    "    #input layer\n",
    "    input_syn = {\"name\":\"input layer\"}\n",
    "    input_syn[\"connection\"] = create_connection(len(input_data[0]),len(input_data))\n",
    "    nn.append(input_syn)\n",
    "    #hidden layers\n",
    "    for i in range(num_hidden_layers):\n",
    "        syn = {\"name\":i}\n",
    "        syn[\"connection\"] = create_connection(len(input_data),len(input_data))\n",
    "        nn.append(syn)\n",
    "    #output_layer\n",
    "    syn = {\"name\":\"output layer\"}\n",
    "    syn[\"connection\"] = create_connection(len(output_data),len(output_data[0]))\n",
    "    nn.append(syn)\n",
    "    nn.append({\"name\":\"output data\",\"connection\":output_data})\n",
    "    return nn\n",
    "```\n",
    "\n",
    "Creating a connection really means creating a randomly initialized matrix whose values will update over the course of back propagation.\n",
    "\n",
    "As you can see, the shapes of our matrices will depend on the previous layer connecting to it - this is because we can only do matrix multiplication with composable shapes.\n",
    "\n",
    "Our so called \"synapse\" layers are actually the weight matrices that we will use to join each layer.\n",
    "\n",
    "## Forward Propagation\n",
    "\n",
    "Next we have forward propagation, in this stage we apply what's called an activation function to each matrix multiplication.  This activation function is a non-linearity, which takes us from the trivial case of just composing linear regression ontop of itself, to being able to learn many different kinds of patterns.  The activation functions are part of the power of neural networks, they allow us to traverse a vast set of models, which is what makes neural networks so flexible - they aren't just one model, they are in fact an interchangable set of many different models, cobbled together.\n",
    "\n",
    "Notice also, that the synapses are sort of like the glue connecting one layer to the next for forward propagation.  \n",
    "\n",
    "```python\n",
    "def forward_propagate(synapses):\n",
    "    layers = [synapses[0][\"connection\"]]\n",
    "    for ind,synapse in enumerate(synapses[:-1]):\n",
    "        if ind == 0: continue\n",
    "        layers.append(\n",
    "            nonlin(np.dot(layers[ind-1],synapse[\"connection\"]))\n",
    "        )\n",
    "    return layers\n",
    "```\n",
    "\n",
    "## Back Propagation\n",
    "\n",
    "Back propagation is the second large innovation in neural networks we will discuss.  The way a neural network is trained, via back propagation means the interplay between the final output and error has a sophisticated and complex relationship with the weight space.  Because the weights at the end of the neural network, influence the nodes at the beginning of the neural network when error is propagated backwards. \n",
    "\n",
    "```python\n",
    "def back_propagate(layers,synapses):\n",
    "    errors = [synapses[-1][\"connection\"] - layers[-1]]\n",
    "    synapses_index = -1\n",
    "    layers_index = -1\n",
    "    errors_index = 0\n",
    "    deltas_index = 0\n",
    "    deltas = []\n",
    "    while len(layers) - abs(layers_index) > 0:\n",
    "        deltas.append(errors[errors_index]*nonlin(layers[layers_index],deriv=True))\n",
    "        synapses_index -= 1\n",
    "        layers_index -= 1\n",
    "        errors.append(deltas[deltas_index].dot(synapses[synapses_index][\"connection\"].T))\n",
    "        errors_index += 1\n",
    "        deltas_index += 1\n",
    "    synapses_index = -2\n",
    "    layers_index = -2\n",
    "    deltas_index = 0\n",
    "    while len(layers) - abs(layers_index) >= 0:\n",
    "        synapses[synapses_index][\"connection\"] += layers[layers_index].T.dot(deltas[deltas_index])\n",
    "        synapses_index -= 1\n",
    "        layers_index -= 1\n",
    "        deltas_index += 1\n",
    "    return synapses,errors[0]\n",
    "```\n",
    "\n",
    "Because I decided to make my forward propagation as obvious as possible, I pushed a lot of complexity to my back propagation algorithm.  Therefor gleaning insight from this implementation would be too challenging.  And therefore should be avoided.\n",
    "\n",
    "The essence of back propagation is as follows:\n",
    "\n",
    "* the error gets multiplied by the _derivative_ of the activation function at each layer, these form deltas\n",
    "\n",
    "Note: activation functions with easy derivatives are generally a good idea\n",
    "\n",
    "* the deltas get composed via multiplication, updating the weights.\n",
    "\n",
    "This is sort of like how you can store all the row reduction information for solving a system of linear equations in a single matrix.  We should see these matrices as a set of functions in their own right, except they are highly parameterized.\n",
    "\n",
    "## Issues in Neural Networks\n",
    "\n",
    "There are two main types of issues with respect to neural networks:\n",
    "\n",
    "* vanishing gradients\n",
    "* exploding gradients\n",
    "\n",
    "### Vanishing Gradient\n",
    "\n",
    "The vanishing gradient problem is one of early stopping.  During back propagation the gradients can become increadibly small, due to the following simple fact:\n",
    "\n",
    "When you multiple two small numbers they get very small very fast:\n",
    "\n",
    "Example: \n",
    "\n",
    "`0.0005 * 0.0007 = 0.00000035`\n",
    "\n",
    "So essentially what happens is, if the gradients ever head to an increasingly smaller value they will get extremely close to zero very fast.  \n",
    "\n",
    "Unfortunately, the termination condition for training a neural network is similar to what happens when the gradient gets very small, so it's hard to tell if this is because the neural network has learned something or because the neural network accidentaly learned a weight early on that was small, which propagated to the whole system and led to early stopping\n",
    "\n",
    "### Exploding Gradient\n",
    "\n",
    "The exploding gradient has the opposite problem, it's when the gradients become very big very fast, because multiplying two big numbers, also creates a very big number very fast.  If that happens, it's unlikely the neural network will ever converge.\n",
    "\n",
    "Both are issues, but explodiung gradients, luckily are super obvious because there is only one reason why that would happen.  That's why we only train a neural network for a fixed number of epochs.  This way, if indeed we do have a run away gradient, we'll know it happened and won't waste time training forever.\n",
    "\n",
    "\n",
    "## Introduction to Keras\n",
    "\n",
    "Now that we've seen how to work with the machinery of neural networks, let's start making use of our first library.  For this tutorial we will make use of Keras a great high level api for neural networks.\n",
    "\n",
    "Let's start by looking at a simple regression example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.08155820757003\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=6, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "crashes = sns.load_dataset(\"car_crashes\")\n",
    "y = crashes[\"total\"]\n",
    "X = crashes[[\"speeding\", \"alcohol\", \"not_distracted\", \"no_previous\", \"ins_premium\", \"ins_losses\"]]\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\n",
    "estimator.fit(X, y)\n",
    "y_pred = estimator.predict(X)\n",
    "print(metrics.mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, the following:\n",
    "\n",
    "1. we need to set the number of input dimensions to be the same as the number of columns in our X variable.\n",
    "2. we specify the loss in the compile phase as mean squared error.  With Neural networks we can even choose the loss metric!\n",
    "\n",
    "Our model does pretty good!\n",
    "\n",
    "Let's see a simple example of classification before we move onto interpreting our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        50\n",
      "           1       1.00      0.96      0.98        50\n",
      "           2       0.96      1.00      0.98        50\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       150\n",
      "   macro avg       0.99      0.99      0.99       150\n",
      "weighted avg       0.99      0.99      0.99       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=4, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "data = load_iris()\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\n",
    "estimator.fit(data.data, data.target)\n",
    "y_pred = estimator.predict(data.data)\n",
    "print(metrics.classification_report(data.target, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, unsurprisingly our model does quiet well!  But how do we interpret our results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "* https://datascience.stackexchange.com/questions/10000/what-is-the-difference-between-a-dynamic-bayes-network-and-a-hmm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
